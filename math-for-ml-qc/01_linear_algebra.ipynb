{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: folks enter this course with very different familiarities with linear algebra.\n",
    "\n",
    "Section 3 is intended for those who are more familiar with fundamental linear algebra concepts,\n",
    "like matrix multiplication, basis, and rank.\n",
    "\n",
    "If you get stuck or confused in that section, in particular the section on the SVD,\n",
    "feel free to skip it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nF_N9U9pXRN"
   },
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFEk1hwsvjh-"
   },
   "source": [
    "This section includes setup code for the remaining sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OGWHqVCZGuGk",
    "outputId": "4b3030f0-dff8-49de-bbb1-c3006a008206"
   },
   "outputs": [],
   "source": [
    "# importing from standard library\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# importing libraries\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "%env WANDB_NOTEBOOK_NAME=\"nb\" \n",
    "\n",
    "# importing course-specific modules\n",
    "import util\n",
    "import linearalgebra as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1o1BANRBep-E"
   },
   "source": [
    "### Automated Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOZ6bEO5pvdp"
   },
   "source": [
    "This cell sets up an automatic feedback system, or \"autograder\",\n",
    "and (anonymously) reports your progress to the\n",
    "[Weights & Biases](http://wandb.com/)\n",
    "project page that appears in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfrciZac2HBk"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    grader\n",
    "except NameError:\n",
    "    grader = util.WandbTrackedOK(\n",
    "        \"charlesfrye\", \"linearalgebra/config\", \"qc-jul20\", \"linearalgebra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1HeCVWFWXNu"
   },
   "source": [
    "Throughout this notebook, you will see cells like the one below.\n",
    "\n",
    "They immediately follow exercises and will run some tests to check\n",
    "whether your solutions are correct.\n",
    "They are included because quick, specific feedback has been\n",
    "[shown to improve learning](https://files.eric.ed.gov/fulltext/EJ786608.pdf).\n",
    "\n",
    "Run the cell below to see an example of the grader output\n",
    "for an incorrect question.\n",
    "The output will include some setup code,\n",
    "then a series of tests and, towards the bottom, the lines\n",
    "```\n",
    "# Error: expected\n",
    "# x\n",
    "# but got\n",
    "# y\n",
    "```\n",
    "where `x` is the output the autograder expected,\n",
    "and the lines `y` following show what was produced by your code.\n",
    "\n",
    "This particular test is failing because the `dimensions` variable is not defined.\n",
    "Try defining `dimensions = []` and `dimensions = {}`\n",
    "and see how the output changes.\n",
    "You'll see how to correctly answer this question in the following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "iqlc51lX1uwC",
    "outputId": "f1a314f3-b21c-4fc8-e19b-ea1e8b747964"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOvkwqdHHKQ2"
   },
   "source": [
    "# Section 1. Programming with Arrays: Shapes, Dimensions, and Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBYIeXLWualG"
   },
   "source": [
    "This section includes a series of exercises on working with linear algebra in Python.\n",
    "\n",
    "See [this 20 min talk](https://www.youtube.com/watch?v=F3lG9_SxCXk),\n",
    "\"Linear Algebra is Not Like Algebra\",\n",
    "for an introduction to the approach taken in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbqP4KXFcXyU"
   },
   "source": [
    "## Shapes and Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHwmdRqD7v9C"
   },
   "source": [
    "_Note_: The primary library for working with arrays in Python is `numpy`,\n",
    "which is typically `import`ed `as np` (see above).\n",
    "This notebook is not a full introduction to `numpy`.\n",
    "If you'd like a more in-depth tutorial,\n",
    "check out\n",
    "[this online tutorial](https://cs231n.github.io/python-numpy-tutorial/)\n",
    "from a Stanford course on neural networks.\n",
    "\n",
    "The cell below creates four arrays, then prints them out.\n",
    "\n",
    "After reviewing the code and the printed arrays,\n",
    "store the _dimensions_ of these arrays in a dictionary\n",
    "called `dimensions`.\n",
    "Use the variable names as keys.\n",
    "Then execute the cell with `grader.grade` to check your answer.\n",
    "\n",
    "This is an exercise.\n",
    "Exercises in this notebook will be indicated with the format below:\n",
    "\n",
    "#### Store the dimensions of these arrays in a dictionary called `dimensions`.\n",
    "\n",
    "Remember that you can add new cells to this notebook,\n",
    "where you can write additional code.\n",
    "This is particularly helpful when you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcUS8_AV7w3a"
   },
   "outputs": [],
   "source": [
    "A = np.array([1])\n",
    "B = np.array([[1, 2]])\n",
    "C = np.array([[1, 0], [0, 1]])\n",
    "D = np.array([[3], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "KyQcljVn8Fwz",
    "outputId": "75b121fc-c7c2-40d0-93b5-2998a4221809"
   },
   "outputs": [],
   "source": [
    "print(A, \"\\n\"), print(B, \"\\n\"), print(C, \"\\n\"), print(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1iY2vOj7_wj"
   },
   "outputs": [],
   "source": [
    "dimensions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "EQfzesJH8RDe",
    "outputId": "ab014b62-897e-4883-e19d-0d9be641d3cf"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5ZrMzCOvHA5"
   },
   "source": [
    "#### Now, do the same with the _shapes_ of the arrays in a dictionary called `shapes`.\n",
    "\n",
    "The shape of an array is an ordered collection of the number of entries in each dimension.\n",
    "\n",
    "_Note_: shapes are usually represented with tuples in Python: e.g. `(x, y, z)`\n",
    "for an array of three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pgtWZnT8Zzk"
   },
   "outputs": [],
   "source": [
    "shapes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqYpS3a189kT"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXhW-Ge2u11n"
   },
   "source": [
    "Heads up:\n",
    "if you want to know the dimension and shape of an array,\n",
    "use the `.ndim` and `.shape` attributes.\n",
    "Very handy to put in `print`s or `assert`s while debugging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kK6JOk9N8MXS"
   },
   "outputs": [],
   "source": [
    "A.ndim, B.ndim, C.ndim, D.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpCi_eQT8gt8"
   },
   "outputs": [],
   "source": [
    "A.shape, B.shape, C.shape, D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRTdEBjqvybL"
   },
   "source": [
    "The _transpose_ is a common matrix operation.\n",
    "It's so common, it gets represented as an attribute _and_\n",
    "as a single-letter to boot!\n",
    "\n",
    "The transpose of a matrix `M` is written in `numpy` as `M.T`.\n",
    "\n",
    "The cell below prints the transposes of three of the matrices above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFtVyBUw8-d2"
   },
   "outputs": [],
   "source": [
    "print(B.T), print(C.T), print(D.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRTdEBjqvybL"
   },
   "source": [
    "#### Q Can you describe, in your own words, what the transpose does?\n",
    "\n",
    "_Note_: this is an \"in-line\" question.\n",
    "These discussion-style questions will be inter-mingled with coding exercises,\n",
    "but they do not have any attached automatic grading, for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNBzmLANwcg5"
   },
   "source": [
    "Transposition is closely related to matrix shape.\n",
    "\n",
    "#### Define a function, `shape_of_transpose`, that takes in a `matrix` and returns the shape of the transpose of the matrix.\n",
    "\n",
    "Try doing this two ways: with and without transposing `matrix` inside the body of the function.\n",
    "\n",
    "Then, execute the `grader.grade` cell to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFRGECsywLRj"
   },
   "outputs": [],
   "source": [
    "# define the function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUeIApHh9A-U"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoMQZ1BWwekZ"
   },
   "source": [
    "Fun fact: the transpose is also a linear operation,\n",
    "and so can be represented by a tensor!\n",
    "That fact is not useful for implementations of transposition,\n",
    "but it is incredibly useful for linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoSQY_IGyyh7"
   },
   "source": [
    "## Composition and Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dG-0ZLv-A-EK"
   },
   "source": [
    "You're looking through a fellow developer's code and notice that in\n",
    "`their_pipeline`, which appears below,\n",
    "a large amount of data is being passed through four successive matrix operations: in order, the data vectors are multiplied by `W`, then `X`, then `Y`, and finally by `Z`.\n",
    "\n",
    "For simplicity's sake, you want to collapse these four multiplications\n",
    "into one operation, call it `V`.\n",
    "\n",
    "#### Use `np.matmul` to define `V`.\n",
    "\n",
    "_Note_: the `@` symbol can be used, in later versions of Python 3,\n",
    "to represent `np.matmul` just like `*` represents multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O64lKhHVAPfv"
   },
   "outputs": [],
   "source": [
    "W = np.array([[1, 2], [-1, 1]])\n",
    "X = np.array([[1/10, 1/5], [1/4, 1]])\n",
    "Y = np.array([[3, 1], [0.1, 0]])\n",
    "Z = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWCXh9W7zIqy"
   },
   "outputs": [],
   "source": [
    "def their_pipeline(v):\n",
    "    after_W = np.matmul(W, v)\n",
    "    after_X = np.matmul(X, after_W)\n",
    "    after_Y = Y @ after_X\n",
    "    after_Z = Z @ after_Y\n",
    "    return after_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yIcpYfrzTuI"
   },
   "outputs": [],
   "source": [
    "# define V here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dG-0ZLv-A-EK"
   },
   "source": [
    "Run the `grader.grade` cell to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtaQdiPh9DTc"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9_uzirotQfB"
   },
   "source": [
    "# Section 2. Visualizing Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEueBO2Aj7OX"
   },
   "source": [
    "In two or three dimensions, vectors and linear transformations of vectors\n",
    "can be easily visualized:\n",
    "we can draw a bunch of vectors as points on a two-dimensional screen\n",
    "or in a three-dimensional environment and then _animate_ the transformation.\n",
    "\n",
    "This section does just that,\n",
    "with the aim of building intuition for a bunch of concepts from linear algebra:\n",
    "diagonal matrices, rank, and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftCT7rjsuxLE"
   },
   "source": [
    "See [this YouTube series by 3blue1brown](https://www.youtube.com/watch?v=F3lG9_SxCXk)\n",
    "for more visualizations of linear algebraic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vS901_tlOsl"
   },
   "source": [
    "The animations are set up by the `setup_plot` function in the `la.animate` module.\n",
    "\n",
    "Run the cell below to see the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNWHx-DPfGHX"
   },
   "outputs": [],
   "source": [
    "la.animate.setup_plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHT_SNmveatz"
   },
   "source": [
    "## A Collection of Interesting Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5-z3ZuDlZc9"
   },
   "source": [
    "This cell defines several linear transformations that you might want to visualize.\n",
    "\n",
    "You can also define your own linear transformations,\n",
    "so long as they are represented by two-by-two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUxPYaGpVG8I"
   },
   "outputs": [],
   "source": [
    "eye = [[1, 0],\n",
    "       [0, 1]]\n",
    "\n",
    "scaling1 = [[3, 0],\n",
    "            [0, 3]]\n",
    "\n",
    "scaling2 = [[1/2, 0],\n",
    "            [0, 1/2]]\n",
    "\n",
    "shear = [[1, 2],\n",
    "         [0, 1]]\n",
    "\n",
    "simple_reflection = [[0, 1],\n",
    "                     [1, 0]]\n",
    "\n",
    "negative_reflection = [[0, -1],\n",
    "                       [-1 ,0]]\n",
    "\n",
    "rank0 = [[0, 0],\n",
    "         [0, 0]]\n",
    "\n",
    "rank1 = [[1, 2],\n",
    "         [1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyNkycl7i5BG"
   },
   "source": [
    "## Animating the Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates the animations. See the comments for instructions on use.\n",
    "\n",
    "Some guiding questions appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ct_eu0sEboWh"
   },
   "outputs": [],
   "source": [
    "# This line selects the transformation to visualize.\n",
    "transform = scaling1\n",
    "\n",
    "# This line places a mesh of points to animate:\n",
    "#  either centered at 0 or in the top-right quadrant\n",
    "mesh = la.animate.all_quadrants_mesh\n",
    "# mesh = la.animate.unit_square_mesh\n",
    "\n",
    "# These lines execute the animation code.\n",
    "\n",
    "f, ax, animate, n_frames = la.animate.setup_plot(\n",
    "    transform,\n",
    "    mesh_properties=mesh,\n",
    "    animate_basis=True)\n",
    "\n",
    "anim = animation.FuncAnimation(f, animate, frames=n_frames)\n",
    "HTML(anim.to_jshtml(fps=24, default_mode=\"reflect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Transformations and Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the transformations `scaling1` and `scaling2`.\n",
    "\n",
    "A `scaling` matrix just changes the lengths of vectors, all by the same amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How can we tell a matrix is a scaling matrix just by looking at its entries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis vectors are the columns of an identity matrix (like `eye`).\n",
    "\n",
    "They determine the bottom and left sides of a square with area 1. We call this the \"unit\" square.\n",
    "\n",
    "After the basis vectors are transformed, they still map out a four-sided figure: always a parallelogram, sometimes a rectangle, and sometimes a square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What four-sided shape do the basis vectors define when they're transformed by a scaling matrix? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important property of a matrix is its _determinant_.\n",
    "\n",
    "Determinants tell us, among other things, the ratio of the area of the parallelogram to the area .\n",
    "\n",
    "Use the `unit_square_mesh` to visualize this parallelogram directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Based on this geometric information, what's the determinant of a scaling matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very special \"scaling\" matrix above, called `eye`.\n",
    "\n",
    "Animate its transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What's so special about `eye`?\n",
    "\n",
    "One way of viewing the entries of a matrix is that the columns tell you where your basis vectors go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Explain why `eye` is special from this point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate the matrices `rank1` and `rank0`.\n",
    "\n",
    "These matrices have lower _rank_ than the other transformations.\n",
    "\n",
    "This makes them \"non-invertible\" -- there is no way to \"undo\" the transformation they perform.\n",
    "\n",
    "#### Q Based on the animation, can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, calculate the determinants of the low-rank matrices (with `np.linalg.det`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply np.linalg.det here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What do you get? Give an explanation for this number in terms of the \"area of parallelograms\" interpretation of the determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations and Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call some transformations \"reflections\" because they act like mirrors --\n",
    "they transform points on one side of some axis (aka vector) to their mirror images on the other side.\n",
    "\n",
    "#### What are the axes around which the `negative_reflection` and `simple_reflection` reflect points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function `la.animate.make_rotation` with argument `theta` will make a matrix that rotates the input by an angle `theta`.\n",
    "\n",
    "This works in radians, not degress, so if you want to make a 90 degree rotation, you should input ${\\pi \\over 2}$.\n",
    "In Python, that would be written `np.pi / 2`.\n",
    "\n",
    "Make a 180 degree rotation matrix, name it `rot180`, and visualize its transformation.\n",
    "\n",
    "Note that `rot180` and `negative_reflection` both send the `unit_square_mesh` to the bottom-left quadrant.\n",
    "\n",
    "These transformations are not the same, however.\n",
    "\n",
    "#### Q Compare the animations. How do the transformations `rot180` and `negative_reflection` differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine two transformations into one using matrix multiplication, `np.matmul`.\n",
    "\n",
    "That is, from two transformations, we produce a single transformation, the _composition_ of the two components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose a rotation matrix and a scaling matrix together and observe the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying two complex numbers, or numbers that can have both real and imaginary parts,\n",
    "can be implemented using vectors and matrices.\n",
    "\n",
    "One complex number is represented by a 2-dimensional vector, whose entries represent the real and imaginary components, respectively.\n",
    "\n",
    "The other is represented by a 2x2 matrix that is the composition of a rotation and a scaling matrix.\n",
    "\n",
    "The rotation angle $\\theta$ and scaling value $r$ for creating this matrix come from the polar representation of the complex number: $r\\mathrm{e}^{i\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4j53wEQC0zr"
   },
   "source": [
    "# Section 3. Machine Learning Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array operations, especially matrix multiplications, arise in many places in machine learning.\n",
    "\n",
    "After reviewing how to implement a few simple operations,\n",
    "we'll dive into the _singular value decomposition_,\n",
    "a common method for \"breaking apart\" a single matrix\n",
    "into pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products and the Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AECzBXMAPOQy"
   },
   "source": [
    "The specific case of matrix multiplication where the first matrix\n",
    "is a row vector and the second is a column vector is also called\n",
    "the _dot product_.\n",
    "\n",
    "For more about the uses of matrix multiplication,\n",
    "including implementations in pure Python,\n",
    "numpy, scipy, and in machine learning frameworks,\n",
    "see the **Appendix** to this notebook..\n",
    "\n",
    "#### Implement the dot product for one-dimensional vectors, represented as lists.\n",
    "\n",
    "Try using one or more `for` loops over the elements in the two vectors.\n",
    "You might also use `zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne6_4h4rTDjg"
   },
   "outputs": [],
   "source": [
    "# implement dot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpDajAVu_oGe"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGA7-1J_1m3x"
   },
   "source": [
    "Dot products show up in lots of places.\n",
    "\n",
    "One is in defining the \"size\" or \"length\" of a vector.\n",
    "In mathematics, this is known as the _norm_ of the vector.\n",
    "\n",
    "There are several different notions of length in linear algebra,\n",
    "but the one that corresponds to the physical or _Euclidean_ length or distance\n",
    "we are familiar with comes directly from the dot product.\n",
    "\n",
    "It is written as $\\|v\\|_2$ for a vector $v$\n",
    "and is defined as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWx8ey6sC0zs"
   },
   "source": [
    "$\\|x\\|_2 = \\sqrt{\\mathrm{dot}(x, x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbuuGX8I2Jfi"
   },
   "source": [
    "#### Use your `dot` function to compute the norm of a `vector` by defining a function called `norm`.\n",
    "\n",
    "Hint: you can calculate square roots with the function `np.sqrt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvP0cN5EC0zt"
   },
   "outputs": [],
   "source": [
    "# implement norm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHpP1_CaO298"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFTHc5k_2SuE"
   },
   "source": [
    "Norms are often used to _normalize_ vectors,\n",
    "e.g. data vectors,\n",
    "in machine learning.\n",
    "\n",
    "The normalized vector has the same direction as the input vector\n",
    "but has a norm of `1`.\n",
    "\n",
    "It is obtained by dividing the entries in the vector by the vector's norm.\n",
    "\n",
    "#### Define a function, `normalize`, that normalizes an input `vector`.\n",
    "\n",
    "Make sure to re-use your `norm` function!\n",
    "\n",
    "FYI: there's no way to normalize a vector with norm 0,\n",
    "so don't worry about that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhF0BDuIC0zv"
   },
   "outputs": [],
   "source": [
    "# implement normalize here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xHtuwZBO5HT"
   },
   "outputs": [],
   "source": [
    "grader.grade(\"q07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _singular value decomposition_ or SVD is used to break a matrix $M$ up\n",
    "into three canonical components:\n",
    "\n",
    "- $U$, a \"tall\" matrix: one that is at least as tall (number of rows) as it is wide (number of columns).\n",
    "- $S$, a square matrix with zeros everywhere except on the diagonal.\n",
    "- $V^T$, a \"wide\" matrix: one that is at least as wide (number of columns) as it is tall (number of rows). It is thought of as the transpose of a matrix $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are constructed to satisfy the expression below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "U S V^T = M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "among some others, which we will shortly investigate.\n",
    "\n",
    "This decomposition works as follows:\n",
    "\n",
    "- First, $V^T$ \"throws out\" every vector that $M$ maps to $0$ (these vectors are the _kernel_ of $M$). The result is that every different output of $V^T$ results in a different output at the end of the pipeline. $V^T$, then, contains all the aspects of $M$ that are \"many-to-one\". It also performs an orthogonal operation: a composition of rotations and reflections.\n",
    "- Then, $S$ applies a scaling transformation. It is also often denoted $\\Sigma$. It is called the _singular value matrix_ and its entries the <i>singular values</i>. Its inputs are always the same size as its outputs, and it only maps $0$ to $0$. Therefore, it is a square matrix of full rank and so is invertible.\n",
    "- Finally, $U$ takes the output of $S$ and makes it into an array of the right shape. In general, the output of $S$ can be smaller than the output of $M$, and so $U$ has more rows (i.e. slots in its output) than columns (i.e. slots in its input). $U$ also includes an orthogonal operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine these pieces using the function `la.svd.compact`.\n",
    "\n",
    "This specialized version of the SVD was written for this course.\n",
    "For details on using the version in `numpy`, `np.linalg.svd`,\n",
    "see the notes at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a few matrices to work with: two square matrices with `full_rank` and `low_rank` plus a `wide` and a `tall` matrix.\n",
    "\n",
    "These matrices are all random: the entries are draw by a random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 25\n",
    "rank = 4\n",
    "\n",
    "full_rank = la.random_matrix.SymmetricWigner(size).M\n",
    "low_rank = la.random_matrix.Wishart(size, rank).M\n",
    "\n",
    "wide_matrix = np.squeeze(\n",
    "    [la.random_matrix.generate_random_unit_vector(dim=size) for _ in range(rank)])\n",
    "tall_matrix = np.squeeze(\n",
    "    [la.random_matrix.generate_random_unit_vector(dim=size) for _ in range(rank)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility `la.svd.show_matrix` is also included, so you can visualize the matrices, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(full_rank); la.svd.show_matrix(low_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(wide_matrix); la.svd.show_matrix(tall_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's confirm that the SVD is working as intended.\n",
    "\n",
    "First, we run it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V_T = la.svd.compact(full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we want to check where $USV^T = M$.\n",
    "\n",
    "The cell below uses `array_equal`,\n",
    "which checks whether all of the entries in a pair of arrays are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this cell below, think about what you expect it to return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(U @ S @ V_T, full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Was this surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the SVD promised equality, but `numpy` is telling us we don't have it.\n",
    "What gives?\n",
    "\n",
    "First, computations with floating point numbers are inexact,\n",
    "and rounding errors abound.\n",
    "When working with floats, we should be very cautious\n",
    "about applying any mathematical claims.\n",
    "\n",
    "And second, the SVD is actually computed using an optimization procedure,\n",
    "and so the answer is inexact.\n",
    "\n",
    "So instead of asking whether two arrays are `equal`,\n",
    "we ask if their entries are `close`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U @ S @ V_T, full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd be great if our mathematical results could give us simple tests to run, the way we can define and run tests for other kinds of code, but unfortunately there's typically a large gap between the math in a textbook and what's happening inside our computer.\n",
    "\n",
    "These sorts of issues make numerical computing, including linear algebra, even harder to learn than other kinds of programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the components of the SVD on some examples.\n",
    "\n",
    "They are displayed in order, from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-Rank Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(low_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_svd(*la.svd.compact(low_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SVD is a \"typical case\" for the definition given above.\n",
    "`V_T` is nice and wide, `S` is much smaller than `M`,\n",
    "and `U` is very tall.\n",
    "\n",
    "It is also a typical case for the SVD in practice in ML.\n",
    "In common uses of the SVD,\n",
    "the rank is (approximately) very low,\n",
    "and so the picture above is an accurate mental model.\n",
    "\n",
    "Compare `U` and `V_T` in the graphic above.\n",
    "Though within a matrix there is generally no pattern\n",
    "(because `low_rank` is a random matrix),\n",
    "you might notice a pattern in the entries\n",
    "if you look at both simultaneously.\n",
    "(hint: compare rows of `U` to columns of `V_T`).\n",
    "\n",
    "The cells below confirm that this pattern holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_low, S_low, V_T_low = la.svd.compact(low_rank)\n",
    "\n",
    "V_low = V_T_low.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U_low, V_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full-Rank Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the rank is not low,\n",
    "the SVD looks rather different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(full_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_svd(U, S, V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $U$ and $V^T$ are both square.\n",
    "\n",
    "Review the definition of tall and wide matrices given above,\n",
    "in the description of the SVD.\n",
    "Under these definitions, a square matrix is actually both tall and wide!\n",
    "These kinds of definitional subtleties are typical in mathematics, so watch out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for a full-rank square matrix,\n",
    "$U$ and $V^T$ are both square,\n",
    "for a low-rank square matrix,\n",
    "$U$ and $V^T$ are both non-square.\n",
    "\n",
    "We can also obtain results between these two extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wide Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(wide_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_svd(*la.svd.compact(wide_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tall Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_matrix(tall_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.svd.show_svd(*la.svd.compact(tall_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the wide matrix, `V_T` is wide but `U` is square.\n",
    "\n",
    "The situation is reversed for the tall matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's close out this section with an exercise.\n",
    "\n",
    "It was stated above that `U` and `V_T` perform orthogonal operations.\n",
    "\n",
    "More precisely, `U` has _columns_ that are orthonormal,\n",
    "while `V_T` has _rows_ that are orthonormal.\n",
    "\n",
    "A collection of vectors is _orthonormal_ if\n",
    "1. the norm of every vector is $1$ (the vectors are normalized)\n",
    "2. the dot product between any pair of different vectors is $0$ (the vectors are orthogonal).\n",
    "\n",
    "#### Write a function, `is_orthonormal`, that checks whether an input `matrix` is orthonormal.\n",
    "\n",
    "You can assume that the input is a two-dimensional `numpy` array, but not that it is square.\n",
    "\n",
    "You should return `True` when the input has orthonormal columns and `False` when it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "- You might use your `norm` and `dot` functions from above.\n",
    "- Don't feel pressure to write this in the shortest possible way. Start by getting something that works, then worry about things like the code quality and the speed.\n",
    "- Writing multiple functions can help. For example, one to check whether all of the norms are 1 and another to check whether all of the dot products are 0. Or one to compute all of the dot products and another to check whether the values are consistent with orthonormality.\n",
    "\n",
    "If you're interested in making a fast method that also wins at [code golf](https://en.wikipedia.org/wiki/Code_golf),\n",
    "consider the hints below.\n",
    "\n",
    "- Is calculating the norm different from calculating a dot product? If so, how?\n",
    "- How do dot products and matrix multiplication interact? How might you implement matrix multiplication as a series of dot products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define is_orthonormal here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.grade(\"q08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Computing the SVD in `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the singular value decomposition, e.g. in `numpy`,\n",
    "there are a few important things to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, there are many different \"flavors\" of SVD, and the flavor implemented by a library might not be the one you're thinking of.\n",
    "\n",
    "For example, by default, `np.linalg.svd` implements something called a \"full\" SVD,\n",
    "in which `U` and `V_T` are square instead of tall and wide.\n",
    "The same decomposition principle is being applied,\n",
    "but the details are slightly different.\n",
    "For example, `U @ S @ V_T` is not directly defined.\n",
    "\n",
    "If the keyword argument `full_matrices`\n",
    "is provided to `np.linalg.svd` with the value `False`,\n",
    "then the flavor of the SVD changes.\n",
    "Now, it is a form of [reduced SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs),\n",
    "but still not the compact SVD.\n",
    "\n",
    "Second, `numpy` actually returns a vector of singular values,\n",
    "which I'll denote `s`,\n",
    "rather than a matrix of singular values.\n",
    "For the full SVD,\n",
    "this means that while `U @ s @ V_T` is undefined,\n",
    "the expression `U @ np.diag(s) @ V_T` is,\n",
    "where `np.diag` converts between a vector and\n",
    "the matrix that has that vectors for its diagonal entries.\n",
    "`np.diag(s)` is the same as the `S` used above.\n",
    "\n",
    "In general, there can be a large gap between the algorithms discussed in classes and math textbooks and those implemented in production-grade libraries like `numpy`.\n",
    "Make sure to read the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pr8UHHtrtbIS"
   },
   "source": [
    "# Appendix: Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a91oDtxhHgfk"
   },
   "source": [
    "## The Matrix Revolution: (almost) Everything is Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7h_DMfMnYxMp"
   },
   "source": [
    "Almost all of the most important operations in linear algebra are based on matrix multiplication.\n",
    "\n",
    "The type signature of matrix multiplication appears below,\n",
    "along with the typical mathematical notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xn8fkxwvd3TE"
   },
   "source": [
    "$$\\mathrm{matmul(x, y)} \\colon \\mathbb{R}^{n \\times m} \\times \\mathbb{R}^{m \\times k} \\rightarrow \\mathbb{R}^{n \\times k}\\\\\n",
    "\\mathrm{matmul}(x, y) = xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fAgOQ9sZDKH"
   },
   "source": [
    "Each section below demonstrates how a key operation in linear algebra is a form of matrix multiplication.\n",
    "\n",
    "The sections also include implementations in pure Python and in several common data science/machine learning libraries.\n",
    "Some sections also include examples of where these operations show up in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TGAGS-PQo58"
   },
   "source": [
    "### Vector-Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stSyOTYfQyJE"
   },
   "source": [
    "#### Dot Product aka Scalar Product aka Inner Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbZuHDl5LpJ1"
   },
   "source": [
    "$$\n",
    "\\mathrm{dot} \\colon \\mathbb{R}^{n\\times 1} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{1\\times 1}  \\\\\n",
    "\\mathrm{dot}(x, y) = \\sum_i x_i \\cdot y_i \\\\ \n",
    "\\mathrm{dot}(x, y) = \\mathrm{matmul}(x^\\top, y)= x^\\top y \\\\ \n",
    "\\mathrm{dot}(x, y) = \\langle x , y \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpySTJzgIRak"
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "vector1 = [random.normalvariate(0, 1) for _ in range(N)]\n",
    "vector2 = [random.normalvariate(0, 1) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltcCuhZbRGHG"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7tNHzqWHuCU"
   },
   "outputs": [],
   "source": [
    "def inner_product(vector1, vector2):\n",
    "    result = 0\n",
    "    for element1, element2 in zip(vector1, vector2):\n",
    "        result += element1 * element2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkTrg2J2RP56"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWMfbncQKfSC"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy(vector1, vector2):\n",
    "    return np.sum(np.multiply(vector1, vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7L-9HFwK8qV"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy_matmul(vector1, vector2):\n",
    "    vector1 = np.atleast_2d(vector1)\n",
    "    vector2 = np.atleast_2d(vector2)\n",
    "\n",
    "    return vector1 @ vector2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPtXV2i0dsB3"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy_dot(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cHVKLyiRVxd"
   },
   "source": [
    "##### TensorFlow and PyTorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmuPbqXMKl7U"
   },
   "outputs": [],
   "source": [
    "def inner_product_tf(vector1, vector2):\n",
    "    return tf.reduce_sum(tf.multiply(vector1, vector2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-7Qow03I44U"
   },
   "outputs": [],
   "source": [
    "def inner_product_tf_tensordot(vector1, vector2):\n",
    "    return tf.tensordot(vector1, vector2, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffJdohPlSvwX"
   },
   "outputs": [],
   "source": [
    "def inner_product_torch(vector1, vector2):\n",
    "    vector1 = torch.Tensor(vector1)\n",
    "    vector2 = torch.Tensor(vector2)\n",
    "\n",
    "    return torch.sum(torch.mul(vector1, vector2)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoYtq2UFRZzd"
   },
   "source": [
    "##### ML Model Examples: Linear Regression, `keras.Dense`, `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiPO_PfvO5Df"
   },
   "outputs": [],
   "source": [
    "def inner_product_sklearn(vector1, vector2):\n",
    "    regression = sklearn.linear_model.LinearRegression(\n",
    "      fit_intercept=False)\n",
    "\n",
    "    regression.coef_ = np.array(vector1)\n",
    "    regression.intercept_ = 0.\n",
    "\n",
    "    return regression.predict(np.atleast_2d(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9ea0NuCNf2S"
   },
   "outputs": [],
   "source": [
    "def inner_product_keras(vector1, vector2):\n",
    "    N = len(vector1)\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "      keras.layers.Dense(\n",
    "          1, activation=None, use_bias=False, input_shape=(N,),\n",
    "          kernel_initializer=keras.initializers.Constant(vector1)))\n",
    "\n",
    "    return model.predict([vector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0vfG9ljU802"
   },
   "outputs": [],
   "source": [
    "def inner_product_torchnn(vector1, vector2):\n",
    "    N = len(vector1)\n",
    "    layer = torch.nn.Linear(N, 1, bias=False)\n",
    "    layer.weight = torch.nn.Parameter(torch.Tensor(vector2))\n",
    "\n",
    "    return layer(torch.Tensor(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rzuDHIeXipj"
   },
   "source": [
    "##### ML Calculation Examples: Norms and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkyJtxUIY-Te"
   },
   "source": [
    "$\\|x\\|_2 = \\sqrt{\\mathrm{dot}(x, x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mS_6uQpQXqzO"
   },
   "outputs": [],
   "source": [
    "def norm(vector):\n",
    "    return np.sqrt(inner_product(vector, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeC0C8leXzKe"
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    vector_norm = norm(vector)\n",
    "    return [element / vector_norm for element in vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TPK5ORXaBYL"
   },
   "source": [
    "$$\\cos(\\angle_{x, y}) = \\frac{\\mathrm{dot}(x, y)}{\\|x\\|_2\\cdot\\|y\\|_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a15Tpk9wXmyG"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    return inner_product(normalize(vector1), normalize(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP9Zv0IabFTX"
   },
   "source": [
    "#### Outer Product aka Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgJ73_psbSBA"
   },
   "source": [
    "$$\n",
    "\\mathrm{outer} \\colon \\mathbb{R}^{n \\times 1} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{n \\times n}  \\\\\n",
    "\\mathrm{outer}(x, y)_{i, j} = x_i \\cdot y_j \\\\ \n",
    "\\mathrm{outer}(x, y) = \\mathrm{matmul}(x, y^\\top) = x y^\\top \\\\\n",
    "\\mathrm{outer}(x, y) = x \\otimes y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVHNppB0cxvX"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkkTXlUwbJYl"
   },
   "outputs": [],
   "source": [
    "def outer_product(vector1, vector2):\n",
    "    result = []\n",
    "    for ii, element1 in enumerate(vector1):\n",
    "        subresult = []\n",
    "    for jj, element2 in enumerate(vector2):\n",
    "        subresult.append(element1 * element2)\n",
    "    result.append(subresult)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qL6lk8_cbMr"
   },
   "outputs": [],
   "source": [
    "outer_product(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqjH-cNDciug"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Mlrmn-c6z_"
   },
   "outputs": [],
   "source": [
    "def outer_product_numpy(vector1, vector2):\n",
    "    return np.outer(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLeYkL6XdFfb"
   },
   "outputs": [],
   "source": [
    "def outer_product_numpy_matmul(vector1, vector2):\n",
    "    vector1 = np.atleast_2d(vector1)\n",
    "    vector2 = np.atleast_2d(vector2)\n",
    "\n",
    "    return vector1.T @ vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlDMb6WQdZ63"
   },
   "outputs": [],
   "source": [
    "outer_product_numpy_matmul(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQvtXRy4ikzd"
   },
   "source": [
    "### Matrix-Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8tj7fcAU0d3"
   },
   "source": [
    "#### Matrix-Vector Multiplication aka Applying Matrices as Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HadJusUWTzfN"
   },
   "source": [
    "Usually, we think of a matrix as a piece of _data_,\n",
    "but we can also think of it as a _function_.\n",
    "\n",
    "When we think of it as a piece of _data_,\n",
    "we often write it mathematically as a\n",
    "\"member of $\\mathbb{R}^{m\\times n}$, the set of arrays\n",
    "with $m$ rows and $n$ columns\", like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0xaG9YGfVci"
   },
   "source": [
    "$$X \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "As a function, we would write it as \"a function that takes in arrays with $n$ entries and returns arrays with $m$ entries\", or:\n",
    "\n",
    "$$X \\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FET9lA0hfuN5"
   },
   "source": [
    "It's useful to think of some corner cases:\n",
    "what if $m == n == 1$?\n",
    "\n",
    "Then we might have\n",
    "$$ 2 \\in \\mathbb{R}$$\n",
    "or\n",
    "$$ 2 \\colon \\mathbb{R} \\rightarrow \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Qzf-LDFgdIh"
   },
   "source": [
    "where $2(x) = 2x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HlPXEEGUeUi"
   },
   "source": [
    "As indicated by this example,\n",
    "the right notion of what it means to \"apply\" a matrix as a function\n",
    "to a vector must be somehow like mutliplication.\n",
    "\n",
    "In fact, the right method isn't exactly multiplication as we are used to it,\n",
    "because the inputs and outputs aren't the same shape,\n",
    "but it's close enough that it goes by the same name:\n",
    "matrix-vector multiplication.\n",
    "\n",
    "Its type signature and definition are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCt7ys1Di_Eg"
   },
   "source": [
    "$$\\mathrm{matvec}(X, y) \\colon \\mathbb{R}^{m \\times n} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{m \\times 1}\\\\\n",
    "\\mathrm{matvec}(X, y) = \\mathrm{matmul}(X, y) = Xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19KemrmGjmmV"
   },
   "source": [
    "$$\n",
    "\\mathrm{matvec}(X, y)_i = \\sum_j X_{i,j} y_j\\\\\n",
    "\\mathrm{matvec}(X, y)_i = {X_{i, \\colon}} y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEsfNSGdPR0T"
   },
   "source": [
    "$$\n",
    "\\mathrm{matvec}(X, y) = \\sum_j {X_{\\colon, j}} y_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvLeej95irfN"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UahTVtGkdH8"
   },
   "outputs": [],
   "source": [
    "def matvec(matrix, vector):\n",
    "    result = []\n",
    "    for row in matrix:\n",
    "        result.append(inner_product(row, vector))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEmpnnKDMGL4"
   },
   "outputs": [],
   "source": [
    "def matvec_sum(matrix, vector):\n",
    "    result = []\n",
    "    for ii, row in enumerate(matrix):\n",
    "        result_ii = 0\n",
    "        for jj, element in enumerate(vector):\n",
    "            result_ii += row[jj] * element \n",
    "        result.append(result_ii)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBD7469gkulZ"
   },
   "outputs": [],
   "source": [
    "M = 3\n",
    "matrix = [[random.normalvariate(0, 1) for _ in range(N)] for _ in range(M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcD9Udz3ktM4"
   },
   "outputs": [],
   "source": [
    "matvec(matrix, vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1YcRnXyNB-Z"
   },
   "outputs": [],
   "source": [
    "matvec_sum(matrix, vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93OAhM_Ok9sA"
   },
   "outputs": [],
   "source": [
    "np.dot(matrix, vector1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Esyf8eKJLd3n"
   },
   "source": [
    "##### Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OeN9W2mNfZg"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy(matrix, vector):\n",
    "    result = np.zeros_like(np.array(matrix)[:, 0])\n",
    "    for ii, row in enumerate(matrix):\n",
    "        result[ii] = np.sum(np.multiply(row, vector))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNSlz-LtOwze"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy_alternate(matrix, vector):\n",
    "    matrix = np.array(matrix)\n",
    "    result = np.zeros_like(matrix[:, 0])\n",
    "\n",
    "    for jj, column in enumerate(matrix.T):\n",
    "        result += vector[jj] * column\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViH6AUi3lBPA"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy_matmul(matrix, vector):\n",
    "    return np.array(matrix) @ np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOkWFWvwXSvQ"
   },
   "source": [
    "##### Tensorflow and Torch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FioM6qUXhh5"
   },
   "outputs": [],
   "source": [
    "def matvec_tf(matrix, vector):\n",
    "    return tf.reduce_sum(tf.multiply(matrix, vector), axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RY6zZjuKYFT5"
   },
   "outputs": [],
   "source": [
    "def matvec_tf_tensordot(matrix, vector):\n",
    "    return tf.tensordot(matrix, vector, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwzzQnjLhlU5"
   },
   "source": [
    "Notice that this `tensordot` based implementation is identical to the `inner_product_tf` implementation with `tensordot`. That's because both are just implementing `matmul`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6mf-i-AY9Eq"
   },
   "outputs": [],
   "source": [
    "def matvec_torch(matrix, vector):\n",
    "    matrix = torch.Tensor(matrix)\n",
    "    vector = torch.Tensor(vector)\n",
    "\n",
    "    return torch.sum(torch.mul(matrix, vector), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G91rcKr8a9TJ"
   },
   "source": [
    "##### ML Model Examples: Multiple Linear Regression, `keras.Dense`, `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekUPBPo3heQi"
   },
   "source": [
    "Note similarity to dot products above.\n",
    "Not a coincidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoDOCdJsaekh"
   },
   "outputs": [],
   "source": [
    "def matvec_sklearn(matrix, vector):\n",
    "    regression = sklearn.linear_model.LinearRegression(\n",
    "      fit_intercept=False)\n",
    "\n",
    "    regression.coef_ = np.array(matrix)\n",
    "    regression.intercept_ = 0.\n",
    "\n",
    "    return regression.predict(np.atleast_2d(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "As09KdXkbMqy"
   },
   "outputs": [],
   "source": [
    "def matvec_keras(matrix, vector):\n",
    "    N, M = len(matrix), len(matrix[0])\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "      keras.layers.Dense(\n",
    "          N, activation=None, use_bias=False, input_shape=(M,),\n",
    "          kernel_initializer=keras.initializers.Constant(np.array(matrix).T)))\n",
    "\n",
    "    return model.predict([vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDIAbUjfg-6p"
   },
   "source": [
    "Gotcha: `Keras` does all of its matrix multiplications \"from the right\",\n",
    "as in $y^\\top X$ rather than $Xy$,\n",
    "so we need to use the transpose here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJGg2vk3bPRP"
   },
   "outputs": [],
   "source": [
    "def matvec_torchnn(matrix, vector):\n",
    "    N, M = len(matrix), len(matrix[0])\n",
    "    layer = torch.nn.Linear(N, M, bias=False)\n",
    "    layer.weight = torch.nn.Parameter(torch.Tensor(matrix))\n",
    "\n",
    "    return layer(torch.Tensor(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUH035ymisyY"
   },
   "source": [
    "### Matrix-Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_3uZoUqwMiE"
   },
   "source": [
    "#### Matrix-Matrix Multiplication aka Composing Matrices as Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jP9bmX2ZOVdu"
   },
   "source": [
    "One of the most important features of functions is that they can be **composed**:\n",
    "one function can be applied after another, the output of one used as the input of the next.\n",
    "\n",
    "To compose two functions $f$ and $g$,\n",
    "where\n",
    "\n",
    "$$\n",
    "f\\colon A \\rightarrow B\n",
    "$$\n",
    "and\n",
    "$$\n",
    "g\\colon B \\rightarrow C\n",
    "$$\n",
    "we match the output of one to the inputs of the other,\n",
    "resulting in a new single function,\n",
    "denoted\n",
    "$$\n",
    "g \\circ f \\colon A \\rightarrow C\n",
    "$$\n",
    "where\n",
    "$$\n",
    "g \\circ f(x) = g\\left(f(x)\\right)\n",
    "$$\n",
    "\n",
    "This might be pronounced \"$g$ after $f$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv8RJ-QziyQZ"
   },
   "source": [
    "Similarly, when we combine, or compose, two matrices $X$ and $Y$,\n",
    "the result is a new matrix, $XY$,\n",
    "which we obtain by multiplying the two matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiLsQqsxLHdo"
   },
   "source": [
    "$$\\mathrm{matmul(X, Y)} \\colon \\mathbb{R}^{n \\times m} \\times \\mathbb{R}^{m \\times k} \\rightarrow \\mathbb{R}^{n \\times k}\\\\\n",
    "\\mathrm{matmul}(X, Y) = XY = X \\circ Y$$\n",
    "$$\n",
    "\\mathrm{matmul}(X, Y)_{ij} = \\mathrm{matmul}(X_{i, :}, Y_{:, j})\n",
    "$$\n",
    "$$\n",
    "\\mathrm{matmul}(X, Y)_{ij} = \\sum_r X_{i, r} Y_{r, j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OikYDHfqyr3U"
   },
   "outputs": [],
   "source": [
    "N, M, K = 2, 3, 4\n",
    "matrix1 = [[random.normalvariate(0, 1) for _ in range(M)] for _ in range(N)]\n",
    "matrix2 = [[random.normalvariate(0, 1) for _ in range(K)] for _ in range(M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Y62sVtpv7f0"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSLLDJ9iivxp"
   },
   "outputs": [],
   "source": [
    "def matmul(matrix1, matrix2):\n",
    "    result_array = []\n",
    "    for ii, row in enumerate(matrix1):\n",
    "        result_row = []\n",
    "        for jj in range(len(matrix2[0])):\n",
    "            result = 0\n",
    "            for kk in range(len(matrix2)):\n",
    "                result += matrix1[ii][kk] * matrix2[kk][jj]\n",
    "            result_row.append(result)\n",
    "        result_array.append(result_row)\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBFpRDvRn96S"
   },
   "outputs": [],
   "source": [
    "matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6XqzwF6wZ_T"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTWE6aHlwcuS"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy(matrix1, matrix2):\n",
    "    return np.array(matrix1) @ np.array(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly-5EiDnwhZ8"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy_dot(matrix1, matrix2):\n",
    "    return np.dot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBKr6Mjwwmwb"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy_matmul(matrix1, matrix2):\n",
    "    return np.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9MxeNjqhw0wo"
   },
   "outputs": [],
   "source": [
    "matmul_numpy(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-26dpoNx1ux"
   },
   "outputs": [],
   "source": [
    "matmul_numpy_dot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0affHd0Lx3s7"
   },
   "outputs": [],
   "source": [
    "matmul_numpy_matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCfStwcPy43U"
   },
   "source": [
    "##### Tensorflow and PyTorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9749vRtP0RHS"
   },
   "outputs": [],
   "source": [
    "def matmul_tf(matrix1, matrix2):\n",
    "    return tf.matmul(matrix1, matrix2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yMPWF13zHOk"
   },
   "outputs": [],
   "source": [
    "def matmul_tf_tensordot(matrix1, matrix2):\n",
    "    return tf.tensordot(matrix1, matrix2, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skWXzh7QzLFL"
   },
   "outputs": [],
   "source": [
    "matmul_tf_tensordot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLlbzgb83Mr8"
   },
   "outputs": [],
   "source": [
    "def matmul_torch(matrix1, matrix2):\n",
    "    matrix1 = torch.Tensor(matrix1)\n",
    "    matrix2 = torch.Tensor(matrix2)\n",
    "\n",
    "    return torch.matmul(matrix1, matrix2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VD8oFk9v0fA4"
   },
   "outputs": [],
   "source": [
    "def matmul_torch_tensordot(matrix1, matrix2):\n",
    "    matrix1 = torch.Tensor(matrix1)\n",
    "    matrix2 = torch.Tensor(matrix2)\n",
    "\n",
    "    return torch.tensordot(matrix1, matrix2, dims=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TzC1e2A0t3M"
   },
   "outputs": [],
   "source": [
    "matmul_torch(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVvAQWST4RJO"
   },
   "outputs": [],
   "source": [
    "matmul_torch_tensordot(matrix1, matrix2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qOvkwqdHHKQ2",
    "K4j53wEQC0zr",
    "K9_uzirotQfB",
    "SHT_SNmveatz",
    "uyNkycl7i5BG",
    "a91oDtxhHgfk",
    "5TGAGS-PQo58",
    "bQvtXRy4ikzd",
    "G8tj7fcAU0d3",
    "LvLeej95irfN",
    "Esyf8eKJLd3n",
    "uOkWFWvwXSvQ",
    "G91rcKr8a9TJ",
    "BUH035ymisyY",
    "m_3uZoUqwMiE",
    "5Y62sVtpv7f0",
    "o6XqzwF6wZ_T"
   ],
   "name": "Colab - Math4ML I: Linear Algebra",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
